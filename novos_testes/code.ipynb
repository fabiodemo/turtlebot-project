{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pyglet in /home/fabio/mambaforge/envs/turtlebot/lib/python3.8/site-packages (2.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyglet.clock' has no attribute 'set_fps_limit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39marm_env\u001b[39;00m \u001b[39mimport\u001b[39;00m ArmEnv\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github/turtlebot-project/novos_testes/arm_env.py:16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyglet\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m pyglet\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mset_fps_limit(\u001b[39m10000\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mArmEnv\u001b[39;00m(\u001b[39mobject\u001b[39m):\n\u001b[1;32m     20\u001b[0m     action_bound \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyglet.clock' has no attribute 'set_fps_limit'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from arm_env import ArmEnv\n",
    "import gym\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import deque\n",
    "import random\n",
    "# from torch.autograd import Variable\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data*(1.0 - tau)+ param.data*tau)\n",
    "\n",
    "def hard_update(target,source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNoise:\n",
    "    # Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "    def __init__(self, action_dim, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.X = np.ones(self.action_dim)*self.mu\n",
    "        \n",
    "    def reset(self):\n",
    "        self.X = np.ones(self.action_dim)*self.mu\n",
    "    \n",
    "    def sample(self):\n",
    "        dx = self.theta*(self.mu - self.X)\n",
    "        dx = dx + self.sigma*np.random.randn(len(self.X))\n",
    "        self.X = self.X + dx\n",
    "        print(self.X)\n",
    "        return self.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = ActionNoise(2)\n",
    "# print(noise.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 0.003\n",
    "def fanin_init(size, fanin=None):\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1./np.sqrt(fanin)\n",
    "    return torch.Tensor(size).uniform_(-v,v)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.state_dim = state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, 150)\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        \n",
    "        self.fa1 = nn.Linear(action_dim, 150)\n",
    "        self.fa1.weight.data = fanin_init(self.fa1.weight.data.size())\n",
    "        \n",
    "        self.fca1 = nn.Linear(300, 300)\n",
    "        self.fca1.weight.data = fanin_init(self.fca1.weight.data.size())\n",
    "        \n",
    "        self.fca2 = nn.Linear(300, 1)\n",
    "        self.fca2.weight.data.uniform_(-EPS, EPS)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        xs = torch.relu(self.fc1(state))\n",
    "        xa = torch.relu(self.fa1(action))\n",
    "        x = torch.cat((xs,xa), dim=1)\n",
    "        x = torch.relu(self.fca1(x))\n",
    "        vs = self.fca2(x)\n",
    "        \n",
    "        return vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_lim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_dim = state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_lim = action_lim\n",
    "        \n",
    "        self.fa1 = nn.Linear(state_dim, 300)\n",
    "        self.fa1.weight.data = fanin_init(self.fa1.weight.data.size())\n",
    "        \n",
    "        self.fa2 = nn.Linear(300, 300)\n",
    "        self.fa2.weight.data = fanin_init(self.fa2.weight.data.size())\n",
    "        \n",
    "        self.fa3 = nn.Linear(300, action_dim)\n",
    "        self.fa3.weight.data.uniform_(-EPS,EPS)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fa1(state))\n",
    "        x = torch.relu(self.fa2(x))\n",
    "        action = torch.tanh(self.fa3(x))\n",
    "        \n",
    "        action = action * self.action_lim\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0175, -0.0217]], grad_fn=<MulBackward>)\n"
     ]
    }
   ],
   "source": [
    "t1 = Actor(7,2,1)\n",
    "print(t1.forward(torch.tensor([[2.,4.,5.,5.,5.,5.,5.]],dtype=torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.maxSize = size\n",
    "        self.len = 0\n",
    "        \n",
    "    def sample(self, count):\n",
    "        batch = []\n",
    "        count = min(count, self.len)\n",
    "        batch = random.sample(self.buffer, count)\n",
    "        \n",
    "        s_array = np.float32([array[0] for array in batch])\n",
    "        a_array = np.float32([array[1] for array in batch])\n",
    "        r_array = np.float32([array[2] for array in batch])\n",
    "        new_s_array = np.float32([array[3] for array in batch])\n",
    "        \n",
    "        return s_array, a_array, r_array, new_s_array\n",
    "    \n",
    "    def len(self):\n",
    "        return self.len\n",
    "    \n",
    "    def add(self, s, a, r, new_s):\n",
    "        transition = (s, a, r, new_s)\n",
    "        self.len += 1 \n",
    "        if self.len > self.maxSize:\n",
    "            self.len = self.maxSize\n",
    "        self.buffer.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "\n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, action_lim, ram):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_lim = action_lim\n",
    "        self.ram = ram\n",
    "        #self.iter = 0 \n",
    "        self.noise = ActionNoise(self.action_dim)\n",
    "        \n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.action_lim)\n",
    "        self.target_actor = Actor(self.state_dim, self.action_dim, self.action_lim)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), LEARNING_RATE)\n",
    "        \n",
    "        self.critic = Critic(self.state_dim, self.action_dim)\n",
    "        self.target_critic = Critic(self.state_dim, self.action_dim)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), LEARNING_RATE)\n",
    "        \n",
    "        hard_update(self.target_actor, self.actor)\n",
    "        hard_update(self.target_critic, self.critic)\n",
    "        \n",
    "    def get_exploitation_action(self,state):\n",
    "        state = torch.from_numpy(state)\n",
    "        action = self.target_actor.forward(state).detach()\n",
    "        return action.data.numpy()\n",
    "        \n",
    "    def get_exploration_action(self, state):\n",
    "        state = torch.from_numpy(state)\n",
    "        action = self.actor.forward(state).detach()\n",
    "        new_action = action.data.numpy() #+(self.noise.sample() * self.action_lim)\n",
    "        return new_action\n",
    "    \n",
    "    def optimizer(self):\n",
    "        s_sample, a_sample, r_sample, new_s_sample = ram.sample(BATCH_SIZE)\n",
    "        \n",
    "        s_sample = torch.from_numpy(s_sample)\n",
    "        a_sample = torch.from_numpy(a_sample)\n",
    "        r_sample = torch.from_numpy(r_sample)\n",
    "        new_s_sample = torch.from_numpy(new_s_sample)\n",
    "        \n",
    "        #-------------- optimize critic\n",
    "        \n",
    "        a_target = self.target_actor.forward(new_s_sample).detach()\n",
    "        next_value = torch.squeeze(self.target_critic.forward(new_s_sample, a_target).detach())\n",
    "        # y_exp = r _ gamma*Q'(s', P'(s'))\n",
    "        y_expected = r_sample + GAMMA*next_value\n",
    "        # y_pred = Q(s,a)\n",
    "        y_predicted = torch.squeeze(self.critic.forward(s_sample, a_sample))\n",
    "        loss_critic = F.smooth_l1_loss(y_predicted, y_expected)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        #------------ optimize actor\n",
    "        pred_a_sample = self.actor.forward(s_sample)\n",
    "        loss_actor = -1*torch.sum(self.critic.forward(s_sample, pred_a_sample))\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        soft_update(self.target_actor, self.actor, TAU)\n",
    "        soft_update(self.target_critic, self.critic, TAU)\n",
    "    \n",
    "    def save_models(self, episode_count):\n",
    "        \n",
    "        torch.save(self.target_actor.state_dict(), './models/'+str(episode_count)+ '_actor.pt')\n",
    "        torch.save(self.target_critic.state_dict(), './models/'+str(episode_count)+ '_critic.pt')\n",
    "        print('****Models saved***')\n",
    "        \n",
    "    def load_models(self, episode):\n",
    "        \n",
    "        self.actor.load_state_dict(torch.load('./models/'+str(episode)+ '_actor.pt'))\n",
    "        self.critic.load_state_dict(torch.load('./models/'+str(episode)+ '_critic.pt'))\n",
    "        hard_update(self.target_actor, self.actor)\n",
    "        hard_update(self.target_critic, self.critic)\n",
    "        print('***Models load***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploration_rate 1\n",
      "State Dimensions: 7\n",
      "Action Dimensions: 2\n",
      "Action Max: 1\n",
      "Episode: 0\n",
      "reward per episode: -358.99206903837245\n",
      "explore: 1\n",
      "****Models saved***\n",
      "Episode: 1\n",
      "reward per episode: -396.9478573479679\n",
      "explore: 1\n",
      "Episode: 2\n",
      "reward per episode: -306.05695491534806\n",
      "explore: 1\n",
      "Episode: 3\n",
      "reward per episode: -202.03462347352755\n",
      "explore: 1\n",
      "Episode: 4\n",
      "reward per episode: -556.9370078844258\n",
      "explore: 1\n",
      "Episode: 5\n",
      "reward per episode: -410.1258494867277\n",
      "explore: 1\n",
      "Episode: 6\n",
      "reward per episode: -384.70537241509146\n",
      "explore: 1\n",
      "Episode: 7\n",
      "reward per episode: -401.7875729377462\n",
      "explore: 1\n",
      "Episode: 8\n",
      "reward per episode: -660.0395860313192\n",
      "explore: 1\n",
      "Episode: 9\n",
      "reward per episode: -429.7567153570176\n",
      "explore: 1\n",
      "Episode: 10\n",
      "reward per episode: -382.4714798772276\n",
      "explore: 1\n",
      "Episode: 11\n",
      "reward per episode: -447.25152159961215\n",
      "explore: 1\n",
      "Episode: 12\n",
      "reward per episode: -623.4401487248325\n",
      "explore: 1\n",
      "Episode: 13\n",
      "reward per episode: -723.5112820596958\n",
      "explore: 0.9999\n",
      "Episode: 14\n",
      "reward per episode: -429.47251168196834\n",
      "explore: 0.9511319235669539\n",
      "Episode: 15\n",
      "reward per episode: -333.69388263091275\n",
      "explore: 0.9047424102692004\n",
      "Episode: 16\n",
      "reward per episode: -291.5774554706341\n",
      "explore: 0.8606154505570021\n",
      "Episode: 17\n",
      "reward per episode: -331.3859593971351\n",
      "explore: 0.8186406930090225\n",
      "Episode: 18\n",
      "reward per episode: -332.0770800764635\n",
      "explore: 0.7787131683686925\n",
      "Episode: 19\n",
      "reward per episode: -331.7893482901079\n",
      "explore: 0.7407330270401349\n",
      "Episode: 20\n",
      "reward per episode: -265.8926400876093\n",
      "explore: 0.7046052893871948\n",
      "Episode: 21\n",
      "reward per episode: -209.21775686795993\n",
      "explore: 0.6702396082111141\n",
      "Episode: 22\n",
      "reward per episode: -423.5563685707277\n",
      "explore: 0.6375500428128791\n",
      "Episode: 23\n",
      "reward per episode: -212.67563341305078\n",
      "explore: 0.6064548440752141\n",
      "Episode: 24\n",
      "reward per episode: -169.86530692729363\n",
      "explore: 0.576876250026757\n",
      "Episode: 25\n",
      "reward per episode: -351.46753121363315\n",
      "explore: 0.548740291377179\n",
      "Episode: 26\n",
      "reward per episode: -161.06558974437638\n",
      "explore: 0.5219766065369207\n",
      "Episode: 27\n",
      "reward per episode: -116.14007436512037\n",
      "explore: 0.4965182656589779\n",
      "Episode: 28\n",
      "reward per episode: 8.965877488904454\n",
      "explore: 0.47230160326267795\n",
      "Episode: 29\n",
      "reward per episode: -89.93103422550249\n",
      "explore: 0.4492660590208893\n",
      "Episode: 30\n",
      "reward per episode: 140.7909164382869\n",
      "explore: 0.42735402631251446\n",
      "Episode: 31\n",
      "reward per episode: 172.77932457129538\n",
      "explore: 0.406510708161521\n",
      "Episode: 32\n",
      "reward per episode: 126.40597925715099\n",
      "explore: 0.39292099882814074\n",
      "Episode: 33\n",
      "reward per episode: 137.8789759530052\n",
      "explore: 0.3849477249814571\n",
      "Episode: 34\n",
      "reward per episode: 352.29030246126314\n",
      "explore: 0.3661726874030757\n",
      "Episode: 35\n",
      "reward per episode: -67.84512373069687\n",
      "explore: 0.34831336386375455\n",
      "Episode: 36\n",
      "reward per episode: 166.96973825415634\n",
      "explore: 0.34012097236016087\n",
      "Episode: 37\n",
      "reward per episode: 133.3337601903081\n",
      "explore: 0.3235322679131739\n",
      "Episode: 38\n",
      "reward per episode: 81.36228569602997\n",
      "explore: 0.30775264358059373\n",
      "Episode: 39\n",
      "reward per episode: -105.83104460916071\n",
      "explore: 0.2927426381354385\n",
      "Episode: 40\n",
      "reward per episode: 42.4301977727385\n",
      "explore: 0.2902647477684568\n",
      "Episode: 41\n",
      "reward per episode: 290.7347388782739\n",
      "explore: 0.2761076786565538\n",
      "Episode: 42\n",
      "reward per episode: 16.359689587288667\n",
      "explore: 0.2626410916213764\n",
      "Episode: 43\n",
      "reward per episode: 150.15506447721273\n",
      "explore: 0.2576977739464934\n",
      "Episode: 44\n",
      "reward per episode: 59.223585348872106\n",
      "explore: 0.25638679558586186\n",
      "Episode: 45\n",
      "reward per episode: 48.04076603140817\n",
      "explore: 0.25444554497193855\n",
      "Episode: 46\n",
      "reward per episode: 64.22339038064136\n",
      "explore: 0.25294866155967616\n",
      "Episode: 47\n",
      "reward per episode: 59.376912459831765\n",
      "explore: 0.2516115133677904\n",
      "Episode: 48\n",
      "reward per episode: 47.86797483190768\n",
      "explore: 0.2499062741978663\n",
      "Episode: 49\n",
      "reward per episode: 52.45634433382185\n",
      "explore: 0.23771760704997216\n",
      "Episode: 50\n",
      "reward per episode: 50.732707040190846\n",
      "explore: 0.23608291998193653\n",
      "Episode: 51\n",
      "reward per episode: -34.67248067713544\n",
      "explore: 0.2245684586495876\n",
      "Episode: 52\n",
      "reward per episode: 125.16775520718446\n",
      "explore: 0.2136155916070091\n",
      "Episode: 53\n",
      "reward per episode: -81.89574008166687\n",
      "explore: 0.2031969282418916\n",
      "Episode: 54\n",
      "reward per episode: 160.493031406404\n",
      "explore: 0.19328641386299297\n",
      "Episode: 55\n",
      "reward per episode: 246.19127894593825\n",
      "explore: 0.18385926454332108\n",
      "Episode: 56\n",
      "reward per episode: -53.189328280776344\n",
      "explore: 0.17489190514120828\n",
      "Episode: 57\n",
      "reward per episode: 173.47495867344372\n",
      "explore: 0.16636191034428113\n",
      "Episode: 58\n",
      "reward per episode: 145.36127584681145\n",
      "explore: 0.16313280406403766\n",
      "Episode: 59\n",
      "reward per episode: 106.32136198486506\n",
      "explore: 0.16115457666634395\n",
      "Episode: 60\n",
      "reward per episode: 180.07484927934377\n",
      "explore: 0.15728552635286036\n",
      "Episode: 61\n",
      "reward per episode: 347.7776715893329\n",
      "explore: 0.15090654269875797\n",
      "Episode: 62\n",
      "reward per episode: 150.07816916684402\n",
      "explore: 0.14584712988271714\n",
      "Episode: 63\n",
      "reward per episode: 35.3247599322149\n",
      "explore: 0.14223174899870572\n",
      "Episode: 64\n",
      "reward per episode: 58.82679123557547\n",
      "explore: 0.14149402675742948\n",
      "Episode: 65\n",
      "reward per episode: 183.1858164912418\n",
      "explore: 0.13459294513754178\n",
      "Episode: 66\n",
      "reward per episode: 50.36011866623625\n",
      "explore: 0.1337075125690224\n",
      "Episode: 67\n",
      "reward per episode: 42.55020117921657\n",
      "explore: 0.13262880245490677\n",
      "Episode: 68\n",
      "reward per episode: 59.649623480430584\n",
      "explore: 0.1319276943237112\n",
      "Episode: 69\n",
      "reward per episode: 244.13612483093368\n",
      "explore: 0.1268178781703847\n",
      "Episode: 70\n",
      "reward per episode: 50.37848391948924\n",
      "explore: 0.1254931909929642\n",
      "Episode: 71\n",
      "reward per episode: 43.53432739716439\n",
      "explore: 0.12449320075915302\n",
      "Episode: 72\n",
      "reward per episode: 140.5339969858149\n",
      "explore: 0.12218669561730515\n",
      "Episode: 73\n",
      "reward per episode: 49.663933625240674\n",
      "explore: 0.12139501825020908\n",
      "Episode: 74\n",
      "reward per episode: 222.89825036209348\n",
      "explore: 0.11806655039229426\n",
      "Episode: 75\n",
      "reward per episode: 55.48490223794399\n",
      "explore: 0.11724292939458848\n",
      "Episode: 76\n",
      "reward per episode: 329.1936582447039\n",
      "explore: 0.11247669298324567\n",
      "Episode: 77\n",
      "reward per episode: 76.12683561615265\n",
      "explore: 0.10699087244084832\n",
      "Episode: 78\n",
      "reward per episode: 58.68018204826484\n",
      "explore: 0.10640400865361911\n",
      "Episode: 79\n",
      "reward per episode: 88.96121161567224\n",
      "explore: 0.10468359674071555\n",
      "Episode: 80\n",
      "reward per episode: 49.710394517426785\n",
      "explore: 0.10393254432434057\n",
      "Episode: 81\n",
      "reward per episode: 30.692265592859243\n",
      "explore: 0.10284690789224235\n",
      "Episode: 82\n",
      "reward per episode: 49.624074923419855\n",
      "explore: 0.10210903279916092\n",
      "Episode: 83\n",
      "reward per episode: 54.141699650523194\n",
      "explore: 0.10145758925955686\n",
      "Episode: 84\n",
      "reward per episode: 51.513175026828485\n",
      "explore: 0.10078006178625741\n",
      "Episode: 85\n",
      "reward per episode: 51.08563539343754\n",
      "explore: 0.10011707050635228\n",
      "Episode: 86\n",
      "reward per episode: 59.358861177411114\n",
      "explore: 0.09960774785698048\n",
      "Episode: 87\n",
      "reward per episode: 44.084476018876195\n",
      "explore: 0.0988535608044178\n",
      "Episode: 88\n",
      "reward per episode: 58.55352485920358\n",
      "explore: 0.09829167032210857\n",
      "Episode: 89\n",
      "reward per episode: 58.99685337739788\n",
      "explore: 0.09778185481439462\n",
      "Episode: 90\n",
      "reward per episode: 47.82630170810704\n",
      "explore: 0.09708031894227637\n",
      "Episode: 91\n",
      "reward per episode: 60.414435162006896\n",
      "explore: 0.09658644507046345\n",
      "Episode: 92\n",
      "reward per episode: 48.67058380021993\n",
      "explore: 0.0958263803593104\n",
      "Episode: 93\n",
      "reward per episode: 56.23977246779147\n",
      "explore: 0.09526264119369522\n",
      "Episode: 94\n",
      "reward per episode: 51.972729186347834\n",
      "explore: 0.09464541133424273\n",
      "Episode: 95\n",
      "reward per episode: 51.56280195934075\n",
      "explore: 0.09402277743934875\n",
      "Episode: 96\n",
      "reward per episode: 54.79775090339319\n",
      "explore: 0.09341358095920682\n",
      "Episode: 97\n",
      "reward per episode: 52.82331874039722\n",
      "explore: 0.09278977087322855\n",
      "Episode: 98\n",
      "reward per episode: 59.532037027300674\n",
      "explore: 0.09226234739262708\n",
      "Episode: 99\n",
      "reward per episode: 47.34627086240815\n",
      "explore: 0.09161873405457568\n",
      "Episode: 100\n",
      "reward per episode: 46.504527667381\n",
      "explore: 0.09094322411601308\n",
      "****Models saved***\n",
      "Episode: 101\n",
      "reward per episode: 56.199793314699065\n",
      "explore: 0.09037205431921366\n",
      "Episode: 102\n",
      "reward per episode: 56.58271812283751\n",
      "explore: 0.0898493874695301\n",
      "Episode: 103\n",
      "reward per episode: 47.04221903009292\n",
      "explore: 0.08915125360178898\n",
      "Episode: 104\n",
      "reward per episode: 86.59390610736243\n",
      "explore: 0.08817591524055905\n",
      "Episode: 105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward per episode: 54.37833223040852\n",
      "explore: 0.0876221255538606\n",
      "Episode: 106\n",
      "reward per episode: 54.06392469047317\n",
      "explore: 0.08701088196090408\n",
      "Episode: 107\n",
      "reward per episode: 50.865384130456775\n",
      "explore: 0.08641254359932882\n",
      "Episode: 108\n",
      "reward per episode: 52.07559879698855\n",
      "explore: 0.08585265567818351\n",
      "Episode: 109\n",
      "reward per episode: 156.0207452624351\n",
      "explore: 0.08400122908711925\n",
      "Episode: 110\n",
      "reward per episode: 359.62101679375525\n",
      "explore: 0.0799042410277229\n",
      "Episode: 111\n",
      "reward per episode: 53.189757651875354\n",
      "explore: 0.07937064547391047\n",
      "Episode: 112\n",
      "reward per episode: 5.418845512428197\n",
      "explore: 0.07727938721403174\n",
      "Episode: 113\n",
      "reward per episode: 59.576805407829795\n",
      "explore: 0.07684781060884718\n",
      "Episode: 114\n",
      "reward per episode: 55.83982928660448\n",
      "explore: 0.0763651671926214\n",
      "Episode: 115\n",
      "reward per episode: 15.271210855856374\n",
      "explore: 0.07515298720842614\n",
      "Episode: 116\n",
      "reward per episode: 55.63851250797748\n",
      "explore: 0.07465858609083527\n",
      "Episode: 117\n",
      "reward per episode: 57.128589040925675\n",
      "explore: 0.07421937541762558\n",
      "Episode: 118\n",
      "reward per episode: 58.65033416276245\n",
      "explore: 0.07379750735024652\n",
      "Episode: 119\n",
      "reward per episode: 59.75964136687861\n",
      "explore: 0.07342207944598844\n",
      "Episode: 120\n",
      "reward per episode: 53.71664627879472\n",
      "explore: 0.07293177136421276\n",
      "Episode: 121\n",
      "reward per episode: 60.85534884007128\n",
      "explore: 0.0721266650529365\n",
      "Episode: 122\n",
      "reward per episode: 59.22055090873701\n",
      "explore: 0.07173103758633581\n",
      "Episode: 123\n",
      "reward per episode: 54.148738787065824\n",
      "explore: 0.07099596327654684\n",
      "Episode: 124\n",
      "reward per episode: 57.29352180048062\n",
      "explore: 0.0705712416994139\n",
      "Episode: 125\n",
      "reward per episode: 57.314803476120986\n",
      "explore: 0.0701560765563408\n",
      "Episode: 126\n",
      "reward per episode: 51.50732079358714\n",
      "explore: 0.06970151824544357\n",
      "Episode: 127\n",
      "reward per episode: 49.98527026655218\n",
      "explore: 0.06917376830896117\n",
      "Episode: 128\n",
      "reward per episode: 53.18460601565313\n",
      "explore: 0.06860197367226888\n",
      "Episode: 129\n",
      "reward per episode: 44.49773342936406\n",
      "explore: 0.0680485145477461\n",
      "Episode: 130\n",
      "reward per episode: 59.074342534869686\n",
      "explore: 0.0676752564552402\n",
      "Episode: 131\n",
      "reward per episode: 59.89917805298779\n",
      "explore: 0.06730404574914019\n",
      "Episode: 132\n",
      "reward per episode: 59.75310626837545\n",
      "explore: 0.06693487119917076\n",
      "Episode: 133\n",
      "reward per episode: 51.639909678658775\n",
      "explore: 0.06649453374412195\n",
      "Episode: 134\n",
      "reward per episode: 55.55192426971134\n",
      "explore: 0.06607691418316983\n",
      "Episode: 135\n",
      "reward per episode: 57.58106166947044\n",
      "explore: 0.06568818882040048\n",
      "Episode: 136\n",
      "reward per episode: 142.32669377311052\n",
      "explore: 0.0624843818265594\n",
      "Episode: 137\n",
      "reward per episode: 68.84879964404828\n",
      "explore: 0.06193071063594952\n",
      "Episode: 138\n",
      "reward per episode: 47.74514326142922\n",
      "explore: 0.06148024014227161\n",
      "Episode: 139\n",
      "reward per episode: 58.29597355361661\n",
      "explore: 0.061118556656475025\n",
      "Episode: 140\n",
      "reward per episode: 51.13382796101838\n",
      "explore: 0.06069826925991576\n",
      "Episode: 141\n",
      "reward per episode: 54.770655639275475\n",
      "explore: 0.060317053193999305\n",
      "Episode: 142\n",
      "reward per episode: 56.9004473178223\n",
      "explore: 0.05998020490895652\n",
      "Episode: 143\n",
      "reward per episode: 58.19098734759491\n",
      "explore: 0.059639273272391824\n",
      "Episode: 144\n",
      "reward per episode: 52.27297882218179\n",
      "explore: 0.05924693060640563\n",
      "Episode: 145\n",
      "reward per episode: 172.2000915759956\n",
      "explore: 0.057009165287938084\n",
      "Episode: 146\n",
      "reward per episode: 54.65310245189464\n",
      "explore: 0.05665678435355841\n",
      "Episode: 147\n",
      "reward per episode: 47.91999622664257\n",
      "explore: 0.056194075286036996\n",
      "Episode: 148\n",
      "reward per episode: 51.76486203191363\n",
      "explore: 0.055829980182686444\n",
      "Episode: 149\n",
      "reward per episode: 53.953014772415386\n",
      "explore: 0.055462697315794555\n",
      "Episode: 150\n",
      "reward per episode: 53.1806194794591\n",
      "explore: 0.055097830654387094\n",
      "Episode: 151\n",
      "reward per episode: 54.71690239829641\n",
      "explore: 0.05475726392357768\n",
      "Episode: 152\n",
      "reward per episode: 37.227799799831594\n",
      "explore: 0.05426663515776978\n",
      "Episode: 153\n",
      "reward per episode: 49.38096279283486\n",
      "explore: 0.05391502840855856\n",
      "Episode: 154\n",
      "reward per episode: 42.76034830578453\n",
      "explore: 0.05345332425229972\n",
      "Episode: 155\n",
      "reward per episode: 49.831906590616704\n",
      "explore: 0.05309105663959137\n",
      "Episode: 156\n",
      "reward per episode: 50.058958824085536\n",
      "explore: 0.05272597108789795\n",
      "Episode: 157\n",
      "reward per episode: 52.334326544568526\n",
      "explore: 0.052363396080700945\n",
      "Episode: 158\n",
      "reward per episode: 53.992350220992364\n",
      "explore: 0.052024120881122654\n",
      "Episode: 159\n",
      "reward per episode: 51.22049011073229\n",
      "explore: 0.05168187522382123\n",
      "Episode: 160\n",
      "reward per episode: 48.59853010930627\n",
      "explore: 0.05131108363394318\n",
      "Episode: 161\n",
      "reward per episode: 51.508055422458334\n",
      "explore: 0.05096843140752541\n",
      "Episode: 162\n",
      "reward per episode: 51.881472703428074\n",
      "explore: 0.050617942284174856\n",
      "Episode: 163\n",
      "reward per episode: 54.90709185389035\n",
      "explore: 0.05028494730732791\n",
      "Episode: 164\n",
      "reward per episode: 49.76209558408172\n",
      "explore: 0.05\n",
      "Episode: 165\n",
      "reward per episode: 51.75890856923289\n",
      "explore: 0.05\n",
      "Episode: 166\n",
      "reward per episode: 51.164252424693345\n",
      "explore: 0.05\n",
      "Episode: 167\n",
      "reward per episode: 104.70213064985195\n",
      "explore: 0.05\n",
      "Episode: 168\n",
      "reward per episode: 50.9352580155072\n",
      "explore: 0.05\n",
      "Episode: 169\n",
      "reward per episode: 58.08865507778794\n",
      "explore: 0.05\n",
      "Episode: 170\n",
      "reward per episode: 48.26519022231622\n",
      "explore: 0.05\n",
      "Episode: 171\n",
      "reward per episode: 49.43025134612951\n",
      "explore: 0.05\n",
      "Episode: 172\n",
      "reward per episode: 56.08184666153659\n",
      "explore: 0.05\n",
      "Episode: 173\n",
      "reward per episode: 127.85295372785347\n",
      "explore: 0.05\n",
      "Episode: 174\n",
      "reward per episode: 53.40669200403239\n",
      "explore: 0.05\n",
      "Episode: 175\n",
      "reward per episode: 55.67857116235669\n",
      "explore: 0.05\n",
      "Episode: 176\n",
      "reward per episode: 56.718461153649145\n",
      "explore: 0.05\n",
      "Episode: 177\n",
      "reward per episode: -42.60560663867336\n",
      "explore: 0.05\n",
      "Episode: 178\n",
      "reward per episode: 49.27320838733173\n",
      "explore: 0.05\n",
      "Episode: 179\n",
      "reward per episode: 86.21839293719296\n",
      "explore: 0.05\n",
      "Episode: 180\n",
      "reward per episode: 46.287744171854854\n",
      "explore: 0.05\n",
      "Episode: 181\n",
      "reward per episode: 53.94000074105796\n",
      "explore: 0.05\n",
      "Episode: 182\n",
      "reward per episode: 49.48550474586283\n",
      "explore: 0.05\n",
      "Episode: 183\n",
      "reward per episode: 54.29366210490208\n",
      "explore: 0.05\n",
      "Episode: 184\n",
      "reward per episode: 59.38237405842334\n",
      "explore: 0.05\n",
      "Episode: 185\n",
      "reward per episode: 49.91856088251326\n",
      "explore: 0.05\n",
      "Episode: 186\n",
      "reward per episode: 54.31563917619992\n",
      "explore: 0.05\n",
      "Episode: 187\n",
      "reward per episode: 112.64639839953881\n",
      "explore: 0.05\n",
      "Episode: 188\n",
      "reward per episode: 51.292086637419814\n",
      "explore: 0.05\n",
      "Episode: 189\n",
      "reward per episode: 58.30847192719436\n",
      "explore: 0.05\n",
      "Episode: 190\n",
      "reward per episode: 86.75320543366443\n",
      "explore: 0.05\n",
      "Episode: 191\n",
      "reward per episode: 56.14427631552743\n",
      "explore: 0.05\n",
      "Episode: 192\n",
      "reward per episode: 57.51217066844646\n",
      "explore: 0.05\n",
      "Episode: 193\n",
      "reward per episode: 50.576438167953356\n",
      "explore: 0.05\n",
      "Episode: 194\n",
      "reward per episode: 67.32999301104144\n",
      "explore: 0.05\n",
      "Episode: 195\n",
      "reward per episode: 55.383961129752585\n",
      "explore: 0.05\n",
      "Episode: 196\n",
      "reward per episode: 59.26830082856735\n",
      "explore: 0.05\n",
      "Episode: 197\n",
      "reward per episode: 52.225849917542114\n",
      "explore: 0.05\n",
      "Episode: 198\n",
      "reward per episode: 47.50435729382235\n",
      "explore: 0.05\n",
      "Episode: 199\n",
      "reward per episode: 54.2423542471757\n",
      "explore: 0.05\n",
      "Episode: 200\n",
      "reward per episode: -35.56463849305501\n",
      "explore: 0.05\n",
      "****Models saved***\n",
      "Episode: 201\n",
      "reward per episode: 50.50008758461786\n",
      "explore: 0.05\n",
      "Episode: 202\n",
      "reward per episode: 50.05213437304518\n",
      "explore: 0.05\n",
      "Episode: 203\n",
      "reward per episode: 55.11148297754716\n",
      "explore: 0.05\n",
      "Episode: 204\n",
      "reward per episode: 96.56784289281185\n",
      "explore: 0.05\n",
      "Episode: 205\n",
      "reward per episode: 54.281592254719435\n",
      "explore: 0.05\n",
      "Episode: 206\n",
      "reward per episode: 56.42050679053311\n",
      "explore: 0.05\n",
      "Episode: 207\n",
      "reward per episode: 42.37153660540896\n",
      "explore: 0.05\n",
      "Episode: 208\n",
      "reward per episode: 59.08918587640391\n",
      "explore: 0.05\n",
      "Episode: 209\n",
      "reward per episode: 35.667783707927676\n",
      "explore: 0.05\n",
      "Episode: 210\n",
      "reward per episode: 99.10931597376829\n",
      "explore: 0.05\n",
      "Episode: 211\n",
      "reward per episode: 46.623983407829535\n",
      "explore: 0.05\n",
      "Episode: 212\n",
      "reward per episode: 59.34946761946914\n",
      "explore: 0.05\n",
      "Episode: 213\n",
      "reward per episode: 77.7192690397031\n",
      "explore: 0.05\n",
      "Episode: 214\n",
      "reward per episode: 45.38977471923022\n",
      "explore: 0.05\n",
      "Episode: 215\n",
      "reward per episode: 57.817716736067226\n",
      "explore: 0.05\n",
      "Episode: 216\n",
      "reward per episode: 57.289174632197934\n",
      "explore: 0.05\n",
      "Episode: 217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward per episode: -43.67676813703995\n",
      "explore: 0.05\n",
      "Episode: 218\n",
      "reward per episode: -92.3075622622499\n",
      "explore: 0.05\n",
      "Episode: 219\n",
      "reward per episode: 276.16869793757655\n",
      "explore: 0.05\n",
      "Episode: 220\n",
      "reward per episode: 59.52882454493164\n",
      "explore: 0.05\n",
      "Episode: 221\n",
      "reward per episode: 52.381638223071775\n",
      "explore: 0.05\n",
      "Episode: 222\n",
      "reward per episode: 53.391267154033784\n",
      "explore: 0.05\n",
      "Episode: 223\n",
      "reward per episode: 56.05973262110652\n",
      "explore: 0.05\n",
      "Episode: 224\n",
      "reward per episode: 43.65210502096192\n",
      "explore: 0.05\n",
      "Episode: 225\n",
      "reward per episode: 95.47221580182207\n",
      "explore: 0.05\n",
      "Episode: 226\n",
      "reward per episode: 86.92566990659168\n",
      "explore: 0.05\n",
      "Episode: 227\n",
      "reward per episode: 57.05822165391078\n",
      "explore: 0.05\n",
      "Episode: 228\n",
      "reward per episode: 51.740118644552545\n",
      "explore: 0.05\n",
      "Episode: 229\n",
      "reward per episode: 51.42699379068891\n",
      "explore: 0.05\n",
      "Episode: 230\n",
      "reward per episode: 56.85511167199937\n",
      "explore: 0.05\n",
      "Episode: 231\n",
      "reward per episode: 59.522112318887444\n",
      "explore: 0.05\n",
      "Episode: 232\n",
      "reward per episode: 42.511718036187034\n",
      "explore: 0.05\n",
      "Episode: 233\n",
      "reward per episode: 58.969145456660115\n",
      "explore: 0.05\n",
      "Episode: 234\n",
      "reward per episode: 59.27869618404064\n",
      "explore: 0.05\n",
      "Episode: 235\n",
      "reward per episode: 52.41668661641306\n",
      "explore: 0.05\n",
      "Episode: 236\n",
      "reward per episode: 58.3289009361897\n",
      "explore: 0.05\n",
      "Episode: 237\n",
      "reward per episode: 58.17512466355067\n",
      "explore: 0.05\n",
      "Episode: 238\n",
      "reward per episode: 56.83640398125282\n",
      "explore: 0.05\n",
      "Episode: 239\n",
      "reward per episode: 51.84303599325879\n",
      "explore: 0.05\n",
      "Episode: 240\n",
      "reward per episode: 53.07526440364421\n",
      "explore: 0.05\n",
      "Episode: 241\n",
      "reward per episode: 56.745429847858944\n",
      "explore: 0.05\n",
      "Episode: 242\n",
      "reward per episode: 58.13635244090243\n",
      "explore: 0.05\n",
      "Episode: 243\n",
      "reward per episode: 270.86888425165233\n",
      "explore: 0.05\n",
      "Episode: 244\n",
      "reward per episode: 50.50373772656982\n",
      "explore: 0.05\n",
      "Episode: 245\n",
      "reward per episode: 51.575886598196476\n",
      "explore: 0.05\n",
      "Episode: 246\n",
      "reward per episode: 58.63974764680826\n",
      "explore: 0.05\n",
      "Episode: 247\n",
      "reward per episode: 50.67767078108446\n",
      "explore: 0.05\n",
      "Episode: 248\n",
      "reward per episode: 51.678242001511116\n",
      "explore: 0.05\n",
      "Episode: 249\n",
      "reward per episode: 37.47147385514836\n",
      "explore: 0.05\n",
      "Episode: 250\n",
      "reward per episode: -8.145625980018997\n",
      "explore: 0.05\n",
      "Episode: 251\n",
      "reward per episode: 50.67593638010317\n",
      "explore: 0.05\n",
      "Episode: 252\n",
      "reward per episode: 58.331972894225295\n",
      "explore: 0.05\n",
      "Episode: 253\n",
      "reward per episode: 59.4049250389383\n",
      "explore: 0.05\n",
      "Episode: 254\n",
      "reward per episode: 51.556259481337435\n",
      "explore: 0.05\n",
      "Episode: 255\n",
      "reward per episode: 49.05141104453946\n",
      "explore: 0.05\n",
      "Episode: 256\n",
      "reward per episode: 48.99168073801185\n",
      "explore: 0.05\n",
      "Episode: 257\n",
      "reward per episode: 16.44118775062907\n",
      "explore: 0.05\n",
      "Episode: 258\n",
      "reward per episode: 75.67968399799456\n",
      "explore: 0.05\n",
      "Episode: 259\n",
      "reward per episode: 44.59699693861741\n",
      "explore: 0.05\n",
      "Episode: 260\n",
      "reward per episode: 51.69434259686923\n",
      "explore: 0.05\n",
      "Episode: 261\n",
      "reward per episode: 121.0342179990918\n",
      "explore: 0.05\n",
      "Episode: 262\n",
      "reward per episode: 34.09498228107512\n",
      "explore: 0.05\n",
      "Episode: 263\n",
      "reward per episode: 57.80345116621544\n",
      "explore: 0.05\n",
      "Episode: 264\n",
      "reward per episode: 125.46125017092969\n",
      "explore: 0.05\n",
      "Episode: 265\n",
      "reward per episode: 51.46221574042741\n",
      "explore: 0.05\n",
      "Episode: 266\n",
      "reward per episode: 50.19381342605476\n",
      "explore: 0.05\n",
      "Episode: 267\n",
      "reward per episode: 53.7801333383231\n",
      "explore: 0.05\n",
      "Episode: 268\n",
      "reward per episode: 49.66102669925435\n",
      "explore: 0.05\n",
      "Episode: 269\n",
      "reward per episode: 60.40279769650097\n",
      "explore: 0.05\n",
      "Episode: 270\n",
      "reward per episode: 58.55221803051455\n",
      "explore: 0.05\n",
      "Episode: 271\n",
      "reward per episode: 53.124821784681075\n",
      "explore: 0.05\n",
      "Episode: 272\n",
      "reward per episode: 58.4584217211701\n",
      "explore: 0.05\n",
      "Episode: 273\n",
      "reward per episode: 48.67642118514615\n",
      "explore: 0.05\n",
      "Episode: 274\n",
      "reward per episode: 58.3622625847089\n",
      "explore: 0.05\n",
      "Episode: 275\n",
      "reward per episode: 88.83532250654346\n",
      "explore: 0.05\n",
      "Episode: 276\n",
      "reward per episode: 53.69698074598619\n",
      "explore: 0.05\n",
      "Episode: 277\n",
      "reward per episode: 58.955273186950436\n",
      "explore: 0.05\n",
      "Episode: 278\n",
      "reward per episode: 129.89053093511552\n",
      "explore: 0.05\n",
      "Episode: 279\n",
      "reward per episode: 52.45811998019411\n",
      "explore: 0.05\n",
      "Episode: 280\n",
      "reward per episode: 50.50612934203639\n",
      "explore: 0.05\n",
      "Episode: 281\n",
      "reward per episode: 55.08632741152186\n",
      "explore: 0.05\n",
      "Episode: 282\n",
      "reward per episode: 51.21915281547272\n",
      "explore: 0.05\n",
      "Episode: 283\n",
      "reward per episode: 52.34917981265496\n",
      "explore: 0.05\n",
      "Episode: 284\n",
      "reward per episode: 129.3759567939727\n",
      "explore: 0.05\n",
      "Episode: 285\n",
      "reward per episode: 52.86087135849341\n",
      "explore: 0.05\n",
      "Episode: 286\n",
      "reward per episode: 337.5181278631724\n",
      "explore: 0.05\n",
      "Episode: 287\n",
      "reward per episode: 48.915389665604465\n",
      "explore: 0.05\n",
      "Episode: 288\n",
      "reward per episode: 51.49429278439419\n",
      "explore: 0.05\n",
      "Episode: 289\n",
      "reward per episode: 193.42136224188235\n",
      "explore: 0.05\n",
      "Episode: 290\n",
      "reward per episode: 31.684205958782904\n",
      "explore: 0.05\n",
      "Episode: 291\n",
      "reward per episode: 39.934481333742355\n",
      "explore: 0.05\n",
      "Episode: 292\n",
      "reward per episode: 44.34722768979831\n",
      "explore: 0.05\n",
      "Episode: 293\n",
      "reward per episode: 72.16360949654798\n",
      "explore: 0.05\n",
      "Episode: 294\n",
      "reward per episode: 51.69693094902004\n",
      "explore: 0.05\n",
      "Episode: 295\n",
      "reward per episode: 228.17604873083508\n",
      "explore: 0.05\n",
      "Episode: 296\n",
      "reward per episode: -41.552773712081766\n",
      "explore: 0.05\n",
      "Episode: 297\n",
      "reward per episode: 78.01146787213435\n",
      "explore: 0.05\n",
      "Episode: 298\n",
      "reward per episode: 189.17176337071177\n",
      "explore: 0.05\n",
      "Episode: 299\n",
      "reward per episode: 92.28708237314885\n",
      "explore: 0.05\n",
      "Episode: 300\n",
      "reward per episode: 86.34392334834384\n",
      "explore: 0.05\n",
      "****Models saved***\n",
      "Episode: 301\n",
      "reward per episode: 72.99894011056469\n",
      "explore: 0.05\n",
      "Episode: 302\n",
      "reward per episode: -49.68675440595811\n",
      "explore: 0.05\n",
      "Episode: 303\n",
      "reward per episode: 51.8404838624368\n",
      "explore: 0.05\n",
      "Episode: 304\n",
      "reward per episode: 50.579492723349034\n",
      "explore: 0.05\n",
      "Episode: 305\n",
      "reward per episode: 43.725152136777055\n",
      "explore: 0.05\n",
      "Episode: 306\n",
      "reward per episode: 43.5874874795147\n",
      "explore: 0.05\n",
      "Episode: 307\n",
      "reward per episode: -31.98300467038912\n",
      "explore: 0.05\n",
      "Episode: 308\n",
      "reward per episode: 59.19346773312104\n",
      "explore: 0.05\n",
      "Episode: 309\n",
      "reward per episode: 94.52685520983009\n",
      "explore: 0.05\n",
      "Episode: 310\n",
      "reward per episode: 49.20701044107351\n",
      "explore: 0.05\n",
      "Episode: 311\n",
      "reward per episode: 45.947613055240794\n",
      "explore: 0.05\n",
      "Episode: 312\n",
      "reward per episode: -34.53362724747828\n",
      "explore: 0.05\n",
      "Episode: 313\n",
      "reward per episode: 33.66020547213256\n",
      "explore: 0.05\n",
      "Episode: 314\n",
      "reward per episode: 65.53852382596398\n",
      "explore: 0.05\n",
      "Episode: 315\n",
      "reward per episode: 58.65098453523002\n",
      "explore: 0.05\n",
      "Episode: 316\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7654cf21aff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMAX_BUFFER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m.9999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;31m#if is_training:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#    trainer.optimizer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7c971c2d331e>\u001b[0m in \u001b[0;36moptimizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mloss_actor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0msoft_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_actor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTAU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_EPISODES = 10001\n",
    "MAX_STEPS = 500\n",
    "MAX_BUFFER = 7000\n",
    "RENDER = True\n",
    "rewards_all_episodes = []\n",
    "\n",
    "exploration_rate = 1\n",
    "is_training = True\n",
    "if is_training:\n",
    "    exploration_rate = 1\n",
    "else:\n",
    "    exploration_rate = 0.1\n",
    "print('exploration_rate', exploration_rate)\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.1\n",
    "exploration_decay_rate = 0.05\n",
    "\n",
    "var = 1\n",
    "\n",
    "MODE = ['easy', 'hard']\n",
    "n_model = 1\n",
    "env = ArmEnv(mode=MODE[n_model])\n",
    "\n",
    "S_DIM = env.state_dim\n",
    "A_DIM = env.action_dim\n",
    "A_MAX = 1\n",
    "\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# S_DIM = env.observation_space.shape[0]\n",
    "# A_DIM = env.action_space.shape[0]\n",
    "# A_MAX = 1\n",
    "\n",
    "print('State Dimensions:', S_DIM)\n",
    "print('Action Dimensions:', A_DIM)\n",
    "print('Action Max:', A_MAX)\n",
    "ram = MemoryBuffer(MAX_BUFFER)\n",
    "trainer = Trainer(S_DIM, A_DIM, A_MAX, ram)\n",
    "#trainer.load_models(1700)\n",
    "\n",
    "\n",
    "for ep in range(MAX_EPISODES):\n",
    "    state = env.reset()\n",
    "    print('Episode:', ep)\n",
    "    \n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "            \n",
    "        state = np.float32(state)\n",
    "        \n",
    "        #exploration_rate_threshold = random.uniform(0,1)\n",
    "        #if exploration_rate_threshold > exploration_rate:\n",
    "        #    action = trainer.get_exploration_action(state)\n",
    "        #else:\n",
    "        #    action = np.array([np.random.uniform(-1,1), np.random.uniform(-1,1)])\n",
    "        action = trainer.get_exploration_action(state)\n",
    "        #action = trainer.get_exploitation_action(state)\n",
    "        action = np.clip(np.random.normal(action,var),-A_MAX,A_MAX)\n",
    "        #if is_training:\n",
    "        #    if ep%2 == 0:\n",
    "        #        action = trainer.get_exploitation_action(state)\n",
    "        #    else:\n",
    "        #        action = trainer.get_exploration_action(state)\n",
    "        #    action = np.clip(action,-A_MAX,A_MAX)\n",
    "        #action = np.clip(action,-A_MAX,A_MAX)\n",
    "        \n",
    "        if not is_training:\n",
    "            action = trainer.get_exploitation_action(state)\n",
    "        #new_state, reward, done, _ = env.step(action)\n",
    "        new_state, reward, done = env.step(action)\n",
    "        \n",
    "        rewards_current_episode += reward        \n",
    "        if done:\n",
    "            new_state = None\n",
    "        else:\n",
    "            new_state = np.float32(new_state)\n",
    "            ram.add(state,action, reward, new_state)\n",
    "        state = new_state\n",
    "        \n",
    "        if ram.len == MAX_BUFFER:\n",
    "            var = max([var*.9999, 0.05])\n",
    "            trainer.optimizer()\n",
    "        #if is_training:\n",
    "        #    trainer.optimizer()\n",
    "\n",
    "        if step == MAX_STEPS-1 or done:\n",
    "            print('reward per episode:', rewards_current_episode)\n",
    "            print('explore:',var)\n",
    "            rewards_all_episodes.append(rewards_current_episode)\n",
    "            break\n",
    "    \n",
    "    exploration_rate = (min_exploration_rate +\n",
    "                (max_exploration_rate - min_exploration_rate)* np.exp(-exploration_decay_rate*ep))\n",
    "    #print(exploration_rate)\n",
    "    # check memory consumption and clear memory\n",
    "    gc.collect()\n",
    "    if ep%100 == 0:\n",
    "        trainer.save_models(ep)\n",
    "\n",
    "print('Completed Episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_actor.pt\t2000_actor.pt\t300_actor.pt\t4100_actor.pt\t5200_actor.pt\r\n",
      "0_critic.pt\t2000_critic.pt\t300_critic.pt\t4100_critic.pt\t5200_critic.pt\r\n",
      "1000_actor.pt\t200_actor.pt\t3100_actor.pt\t4200_actor.pt\t5300_actor.pt\r\n",
      "1000_critic.pt\t200_critic.pt\t3100_critic.pt\t4200_critic.pt\t5300_critic.pt\r\n",
      "100_actor.pt\t2100_actor.pt\t3200_actor.pt\t4300_actor.pt\t5400_actor.pt\r\n",
      "100_critic.pt\t2100_critic.pt\t3200_critic.pt\t4300_critic.pt\t5400_critic.pt\r\n",
      "1100_actor.pt\t2200_actor.pt\t3300_actor.pt\t4400_actor.pt\t5500_actor.pt\r\n",
      "1100_critic.pt\t2200_critic.pt\t3300_critic.pt\t4400_critic.pt\t5500_critic.pt\r\n",
      "1200_actor.pt\t2300_actor.pt\t3400_actor.pt\t4500_actor.pt\t5600_actor.pt\r\n",
      "1200_critic.pt\t2300_critic.pt\t3400_critic.pt\t4500_critic.pt\t5600_critic.pt\r\n",
      "1300_actor.pt\t2400_actor.pt\t3500_actor.pt\t4600_actor.pt\t5700_actor.pt\r\n",
      "1300_critic.pt\t2400_critic.pt\t3500_critic.pt\t4600_critic.pt\t5700_critic.pt\r\n",
      "1400_actor.pt\t2500_actor.pt\t3600_actor.pt\t4700_actor.pt\t600_actor.pt\r\n",
      "1400_critic.pt\t2500_critic.pt\t3600_critic.pt\t4700_critic.pt\t600_critic.pt\r\n",
      "1500_actor.pt\t2600_actor.pt\t3700_actor.pt\t4800_actor.pt\t700_actor.pt\r\n",
      "1500_critic.pt\t2600_critic.pt\t3700_critic.pt\t4800_critic.pt\t700_critic.pt\r\n",
      "1600_actor.pt\t2700_actor.pt\t3800_actor.pt\t4900_actor.pt\t800_actor.pt\r\n",
      "1600_critic.pt\t2700_critic.pt\t3800_critic.pt\t4900_critic.pt\t800_critic.pt\r\n",
      "1700_actor.pt\t2800_actor.pt\t3900_actor.pt\t5000_actor.pt\t900_actor.pt\r\n",
      "1700_critic.pt\t2800_critic.pt\t3900_critic.pt\t5000_critic.pt\t900_critic.pt\r\n",
      "1800_actor.pt\t2900_actor.pt\t4000_actor.pt\t500_actor.pt\r\n",
      "1800_critic.pt\t2900_critic.pt\t4000_critic.pt\t500_critic.pt\r\n",
      "1900_actor.pt\t3000_actor.pt\t400_actor.pt\t5100_actor.pt\r\n",
      "1900_critic.pt\t3000_critic.pt\t400_critic.pt\t5100_critic.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ArmEnv' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1baceacf4cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ArmEnv' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At aqui o cdigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_actor.pt  0_critic.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0176, -0.0171],\n",
       "        [-0.0176, -0.0171],\n",
       "        [-0.0176, -0.0171]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0176, -0.0171, -0.0176],\n",
       "         [-0.0171, -0.0176, -0.0171]]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape(1,2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N_S' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-5a3756d67d03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mN_S\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'N_S' is not defined"
     ]
    }
   ],
   "source": [
    "N_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 1]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_BOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.  , -0.84, -1.07, -0.84, -1.07,  0.16, -0.07])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: [ 1.          0.32766824  0.3522399  -0.00546977 -0.02061271 -0.15\n",
      "  0.5       ]\n",
      "reward -0.021326090371467467\n",
      "done: True\n",
      "state: [ 1.          0.32766824  0.3522399  -0.00546977 -0.02061271 -0.15\n",
      "  0.5       ]\n",
      "reward -0.021326090371467467\n",
      "done: True\n",
      "state: [ 1.          0.32766824  0.3522399  -0.00546977 -0.02061271 -0.15\n",
      "  0.5       ]\n",
      "reward -0.021326090371467467\n",
      "done: True\n",
      "state: [ 1.          0.32766824  0.3522399  -0.00546977 -0.02061271 -0.15\n",
      "  0.5       ]\n",
      "reward -0.021326090371467467\n",
      "done: True\n",
      "state: [ 1.          0.32766824  0.3522399  -0.00546977 -0.02061271 -0.15\n",
      "  0.5       ]\n",
      "reward -0.021326090371467467\n",
      "done: True\n",
      "state: [ 1.          0.32766824  0.3522399  -0.00546977 -0.02061271 -0.15\n",
      "  0.5       ]\n",
      "reward -0.021326090371467467\n",
      "done: True\n",
      "state: [ 1.          0.32766824  0.3522399  -0.00546977 -0.02061271 -0.15\n",
      "  0.5       ]\n",
      "reward -0.021326090371467467\n",
      "done: True\n",
      "state: [ 1.          0.32766824  0.3522399  -0.00546977 -0.02061271 -0.15\n",
      "  0.5       ]\n",
      "reward -0.021326090371467467\n",
      "done: True\n",
      "state: [ 1.          0.32766824  0.3522399  -0.00546977 -0.02061271 -0.15\n",
      "  0.5       ]\n",
      "reward -0.021326090371467467\n",
      "done: True\n",
      "state: [ 1.          0.32766824  0.3522399  -0.00546977 -0.02061271 -0.15\n",
      "  0.5       ]\n",
      "reward -0.021326090371467467\n",
      "done: True\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    env.render()\n",
    "    action = np.array([0,0])\n",
    "    state, reward, done = env.step(action)\n",
    "    print('state:', state)\n",
    "    print('reward', reward)\n",
    "    print('done:', done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
